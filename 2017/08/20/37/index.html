<!doctype html>



  


<html class="theme-next pisces use-motion">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />












  
  
  <link href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.0.1" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Python,ANN," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon/favicon.ico?v=5.0.1" />






<meta name="description" content="By gk_ from machinelearning.co  Understanding how chatbots work is important. A fundamental piece of machinery inside a chat-bot is the text classifier. Let’s look at the inner workings of an artific">
<meta name="keywords" content="Python,ANN">
<meta property="og:type" content="article">
<meta property="og:title" content="Text Classification using Neural Networks">
<meta property="og:url" content="http://codezlee.com/2017/08/20/37/index.html">
<meta property="og:site_name" content="Hello | World">
<meta property="og:description" content="By gk_ from machinelearning.co  Understanding how chatbots work is important. A fundamental piece of machinery inside a chat-bot is the text classifier. Let’s look at the inner workings of an artific">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2017-11-02T20:53:38.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Text Classification using Neural Networks">
<meta name="twitter:description" content="By gk_ from machinelearning.co  Understanding how chatbots work is important. A fundamental piece of machinery inside a chat-bot is the text classifier. Let’s look at the inner workings of an artific">



<script type="text/javascript" id="hexo.configuration">
  var NexT = window.NexT || {};
  var CONFIG = {
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: 0,
      author: 'Author'
    }
  };
</script>




  <link rel="canonical" href="http://codezlee.com/2017/08/20/37/"/>

  <title> Text Classification using Neural Networks | Hello | World </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  










  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/"  class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">Hello | World</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
  <p class="site-subtitle">Everyday is a brand new day.</p>
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            Categories
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            Archives
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-resume">
          <a href="/resume" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-file-text-o"></i> <br />
            
            Resume
          </a>
        </li>
      

      
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  
  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Text Classification using Neural Networks
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            <span class="post-meta-item-icon">
              <i class="fa fa-calendar-o"></i>
            </span>
            <span class="post-meta-item-text">Posted on</span>
            <time itemprop="dateCreated" datetime="2017-08-20T02:31:27+01:00" content="08-19-2017">
              08-19-2017
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp;
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/Tech/" itemprop="url" rel="index">
                    <span itemprop="name">Tech</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
          

          

          
          

          
        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <blockquote>
<p>By <a href="https://machinelearnings.co/text-classification-using-neural-networks-f5cd7b8765c6" target="_blank" rel="external">gk_ from machinelearning.co</a></p>
</blockquote>
<p>Understanding <a href="https://medium.com/@gk_/how-chat-bots-work-dfff656a35e2" target="_blank" rel="external">how chatbots work</a> is important. A fundamental piece of machinery inside a chat-bot is the text classifier. Let’s look at the inner workings of an artificial neural network (ANN) for text classification.</p>
<p>We’ll use 2 layers of neurons (1 hidden layer) and a “bag of words” approach to organizing our training data. Text classification comes in 3 flavors: pattern matching, algorithms, neural nets. While the algorithmic approach using Multinomial Naive Bayes is surprisingly effective, it suffers from 3 fundamental flaws:</p>
<ul>
<li><strong>the algorithm produces a score</strong> rather than a probability. We want a probability to ignore predictions below some threshold. This is akin to a ‘squelch’ dial on a VHF radio.</li>
<li>the algorithm ‘learns’ from examples of what is in a class, but <strong>not what isn’t</strong>. This learning of patterns of what does not belong to a class is often very important.</li>
<li>classes with disproportionately large training sets can create distorted classification scores, forcing the algorithm to <strong>adjust scores relative to class size</strong>. This is not ideal.</li>
</ul>
<p>As with its ‘Naive’ counterpart, this classifier isn’t attempting to understand the meaning of a sentence, it’s trying to classify it. In fact so called “AI chat-bots” do not understand language, but that’s another story.</p>
<p>Let’s examine our text classifier one section at a time. We will take the following steps:</p>
<ol>
<li>refer to libraries we need</li>
<li>provide training data</li>
<li>organize our data</li>
<li>iterate: code + test the results + tune the model</li>
<li>abstract</li>
</ol>
<p>The code is <a href="https://github.com/ugik/notebooks/blob/master/Neural_Network_Classifier.ipynb" target="_blank" rel="external">here</a>, we’re using iPython notebook which is a super productive way of working on data science projects. The code syntax is Python.<br>We begin by importing our natural language toolkit. We need a way to reliably tokenize sentences into words and a way to stem words.</p>
<figure class="highlight haskell"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="meta"># use natural language toolkit</span></div><div class="line"><span class="keyword">import</span> nltk</div><div class="line"><span class="title">from</span> nltk.stem.lancaster <span class="keyword">import</span> LancasterStemmer</div><div class="line"><span class="keyword">import</span> os</div><div class="line"><span class="keyword">import</span> json</div><div class="line"><span class="keyword">import</span> datetime</div><div class="line"><span class="title">stemmer</span> = <span class="type">LancasterStemmer</span>()</div></pre></td></tr></table></figure>
<p>And our training data, 12 sentences belonging to 3 classes (‘intents’).</p>
<figure class="highlight go"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># <span class="number">3</span> classes of training data</div><div class="line">training_data = []</div><div class="line">training_data.<span class="built_in">append</span>(&#123;<span class="string">"class"</span>:<span class="string">"greeting"</span>, <span class="string">"sentence"</span>:<span class="string">"how are you?"</span>&#125;)</div><div class="line">training_data.<span class="built_in">append</span>(&#123;<span class="string">"class"</span>:<span class="string">"greeting"</span>, <span class="string">"sentence"</span>:<span class="string">"how is your day?"</span>&#125;)</div><div class="line">training_data.<span class="built_in">append</span>(&#123;<span class="string">"class"</span>:<span class="string">"greeting"</span>, <span class="string">"sentence"</span>:<span class="string">"good day"</span>&#125;)</div><div class="line">training_data.<span class="built_in">append</span>(&#123;<span class="string">"class"</span>:<span class="string">"greeting"</span>, <span class="string">"sentence"</span>:<span class="string">"how is it going today?"</span>&#125;)</div><div class="line"></div><div class="line">training_data.<span class="built_in">append</span>(&#123;<span class="string">"class"</span>:<span class="string">"goodbye"</span>, <span class="string">"sentence"</span>:<span class="string">"have a nice day"</span>&#125;)</div><div class="line">training_data.<span class="built_in">append</span>(&#123;<span class="string">"class"</span>:<span class="string">"goodbye"</span>, <span class="string">"sentence"</span>:<span class="string">"see you later"</span>&#125;)</div><div class="line">training_data.<span class="built_in">append</span>(&#123;<span class="string">"class"</span>:<span class="string">"goodbye"</span>, <span class="string">"sentence"</span>:<span class="string">"have a nice day"</span>&#125;)</div><div class="line">training_data.<span class="built_in">append</span>(&#123;<span class="string">"class"</span>:<span class="string">"goodbye"</span>, <span class="string">"sentence"</span>:<span class="string">"talk to you soon"</span>&#125;)</div><div class="line"></div><div class="line">training_data.<span class="built_in">append</span>(&#123;<span class="string">"class"</span>:<span class="string">"sandwich"</span>, <span class="string">"sentence"</span>:<span class="string">"make me a sandwich"</span>&#125;)</div><div class="line">training_data.<span class="built_in">append</span>(&#123;<span class="string">"class"</span>:<span class="string">"sandwich"</span>, <span class="string">"sentence"</span>:<span class="string">"can you make a sandwich?"</span>&#125;)</div><div class="line">training_data.<span class="built_in">append</span>(&#123;<span class="string">"class"</span>:<span class="string">"sandwich"</span>, <span class="string">"sentence"</span>:<span class="string">"having a sandwich today?"</span>&#125;)</div><div class="line">training_data.<span class="built_in">append</span>(&#123;<span class="string">"class"</span>:<span class="string">"sandwich"</span>, <span class="string">"sentence"</span>:<span class="string">"what's for lunch?"</span>&#125;)</div><div class="line"><span class="built_in">print</span> (<span class="string">"%s sentences in training data"</span> % <span class="built_in">len</span>(training_data))</div></pre></td></tr></table></figure>
<a id="more"></a>
<p><code>12 sentences in training data</code></p>
<p>We can now organize our data structures for <em>documents</em>, <em>classes</em> and <em>words</em>.</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">words</span> = []</div><div class="line">classes = []</div><div class="line">documents = []</div><div class="line">ignore_words = [<span class="string">'?'</span>]</div><div class="line"><span class="comment"># loop through each sentence in our training data</span></div><div class="line"><span class="keyword">for</span> pattern <span class="keyword">in</span> training_data:</div><div class="line">    <span class="comment"># tokenize each word in the sentence</span></div><div class="line">    w = nltk.word_tokenize(pattern[<span class="string">'sentence'</span>])</div><div class="line">    <span class="comment"># add to our words list</span></div><div class="line">    <span class="keyword">words</span>.extend(w)</div><div class="line">    <span class="comment"># add to documents in our corpus</span></div><div class="line">    documents.append((w, pattern[<span class="string">'class'</span>]))</div><div class="line">    <span class="comment"># add to our classes list</span></div><div class="line">    <span class="keyword">if</span> pattern[<span class="string">'class'</span>] <span class="keyword">not</span> <span class="keyword">in</span> classes:</div><div class="line">        classes.append(pattern[<span class="string">'class'</span>])</div><div class="line"></div><div class="line"><span class="comment"># stem and lower each word and remove duplicates</span></div><div class="line"><span class="keyword">words</span> = [stemmer.stem(w.<span class="built_in">lower</span>()) <span class="keyword">for</span> w <span class="keyword">in</span> <span class="keyword">words</span> <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> ignore_words]</div><div class="line"><span class="keyword">words</span> = list(<span class="built_in">set</span>(<span class="keyword">words</span>))</div><div class="line"></div><div class="line"><span class="comment"># remove duplicates</span></div><div class="line">classes = list(<span class="built_in">set</span>(classes))</div><div class="line"></div><div class="line">print (<span class="built_in">len</span>(documents), <span class="string">"documents"</span>)</div><div class="line">print (<span class="built_in">len</span>(classes), <span class="string">"classes"</span>, classes)</div><div class="line">print (<span class="built_in">len</span>(<span class="keyword">words</span>), <span class="string">"unique stemmed words"</span>, <span class="keyword">words</span>)</div></pre></td></tr></table></figure>
<figure class="highlight basic"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="symbol">12 </span>documents</div><div class="line"><span class="symbol">3 </span>classes [<span class="comment">'greeting', 'goodbye', 'sandwich']</span></div><div class="line"><span class="symbol">26 </span>unique stemmed words [<span class="comment">'sandwich', 'hav', 'a', 'how', 'for', 'ar', 'good', 'mak', 'me', 'it', 'day', 'soon', 'nic', 'lat', 'going', 'you', 'today', 'can', 'lunch', 'is', "'s", 'see', 'to', 'talk', 'yo', 'what']</span></div></pre></td></tr></table></figure>
<p>Notice that each word is stemmed and lower-cased. Stemming helps the machine equate words like “have” and “having”. We don’t care about case.</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line"># create our training data</div><div class="line">training = []</div><div class="line">output = []</div><div class="line"># create <span class="keyword">an</span> empty array <span class="keyword">for</span> our output</div><div class="line">output_empty = [0] * len(classes)</div><div class="line"></div><div class="line"># training <span class="keyword">set</span>, bag of words <span class="keyword">for</span> each sentence</div><div class="line"><span class="keyword">for</span> doc <span class="keyword">in</span> documents:</div><div class="line">    # initialize our bag of words</div><div class="line">    bag = []</div><div class="line">    # <span class="keyword">list</span> of tokenized words <span class="keyword">for</span> the pattern</div><div class="line">    pattern_words = doc[0]</div><div class="line">    # <span class="keyword">stem</span> each <span class="built_in">word</span></div><div class="line">    pattern_words = [stemmer.<span class="keyword">stem</span>(word.<span class="built_in">lower</span>()) <span class="keyword">for</span> word <span class="keyword">in</span> pattern_words]</div><div class="line">    # create our bag of words array</div><div class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> words:</div><div class="line">        bag.<span class="keyword">append</span>(1) <span class="keyword">if</span> w <span class="keyword">in</span> pattern_words <span class="keyword">else</span> bag.<span class="keyword">append</span>(0)</div><div class="line"></div><div class="line">    training.<span class="keyword">append</span>(bag)</div><div class="line">    # output is a '0' <span class="keyword">for</span> each tag and '1' <span class="keyword">for</span> current tag</div><div class="line">    output_row = <span class="keyword">list</span>(output_empty)</div><div class="line">    output_row[classes.<span class="built_in">index</span>(doc[1])] = 1</div><div class="line">    output.<span class="keyword">append</span>(output_row)</div><div class="line"></div><div class="line"># <span class="keyword">sample</span> training/output</div><div class="line">i = 0</div><div class="line">w = documents[i][0]</div><div class="line"><span class="keyword">print</span> ([stemmer.<span class="keyword">stem</span>(word.<span class="built_in">lower</span>()) <span class="keyword">for</span> word <span class="keyword">in</span> w])</div><div class="line"><span class="keyword">print</span> (training[i])</div><div class="line"><span class="keyword">print</span> (output[i])</div></pre></td></tr></table></figure>
<p>Our training data is transformed into “bag of words” for each sentence.</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">['how', 'ar', 'you', '?']</div><div class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div><div class="line">[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>The above step is a classic in text classification: each training sentence is reduced to an array of 0’s and 1’s against the array of unique words in the corpus.</p>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="symbol">'how</span>', <span class="symbol">'are</span>', <span class="symbol">'you</span>', <span class="symbol">'?</span>']</div></pre></td></tr></table></figure>
<p>is stemmed:</p>
<figure class="highlight scheme"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="symbol">'how</span>', <span class="symbol">'ar</span>', <span class="symbol">'you</span>', <span class="symbol">'?</span>']</div></pre></td></tr></table></figure>
<p>then transformed to input: a 1 for each word in the bag (the ? is ignored)</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>and output: the first class</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>]</div></pre></td></tr></table></figure>
<p>Note that a sentence could be given multiple classes, or none.<br>Make sure the above makes sense and play with the code until you grok it.<br>Your first step in machine learning is to have clean data.</p>
<p>Next we have our core functions for our 2-layer neural network.<br>If you are new to artificial neural networks, here is how they work.<br>We use numpy because we want our matrix multiplication to be fast.</p>
<p>We use a sigmoid function to normalize values and its derivative to measure the error rate. Iterating and adjusting until our error rate is acceptably low.<br>Also below we implement our bag-of-words function, transforming an input sentence into an array of 0’s and 1’s. This matches precisely with our transform for training data, always crucial to get this right.</p>
<p>And now we code our neural network training function to create synaptic weights. Don’t get too excited, this is mostly matrix multiplication — from middle-school math class.</p>
<p>credit Andrew Trask <a href="https://iamtrask.github.io//2015/07/12/basic-python-network/" target="_blank" rel="external">https://iamtrask.github.io//2015/07/12/basic-python-network/</a><br>We are now ready to build our neural network model, we will save this as a json structure to represent our synaptic weights.<br>You should experiment with different ‘alpha’ (gradient descent parameter) and see how it affects the error rate. This parameter helps our error adjustment find the lowest error rate:<br>synapse_0 += alpha * synapse_0_weight_update</p>
<p>We use 20 neurons in our hidden layer, you can adjust this easily. These parameters will vary depending on the dimensions and shape of your training data, tune them down to ~10^-3 as a reasonable error rate.</p>
<p>Training with 20 neurons, alpha:0.1, dropout:False<br>Input matrix: 12x26    Output matrix: 1x3<br>delta after 10000 iterations:0.0062613597435<br>delta after 20000 iterations:0.00428296074919<br>delta after 30000 iterations:0.00343930779307<br>delta after 40000 iterations:0.00294648034566<br>delta after 50000 iterations:0.00261467859609<br>delta after 60000 iterations:0.00237219554105<br>delta after 70000 iterations:0.00218521899378<br>delta after 80000 iterations:0.00203547284581<br>delta after 90000 iterations:0.00191211022401<br>delta after 100000 iterations:0.00180823798397<br>saved synapses to: synapses.json<br>processing time: 6.501226902008057 seconds<br>The synapse.json file contains all of our synaptic weights, this is our model.</p>
<p>This classify() function is all that’s needed for the classification once synapse weights have been calculated: ~15 lines of code.<br>The catch: if there’s a change to the training data our model will need to be re-calculated. For a very large dataset this could take a non-insignificant amount of time.<br>We can now generate the probability of a sentence belonging to one (or more) of our classes. This is super fast because it’s dot-product calculation in our previously defined think() function.</p>
<p>sudo make me a sandwich<br> [[‘sandwich’, 0.99917711814437993]]<br>how are you today?<br> [[‘greeting’, 0.99864563257858363]]<br>talk to you tomorrow<br> [[‘goodbye’, 0.95647479275905511]]<br>who are you?<br> [[‘greeting’, 0.8964283843977312]]<br>make me some lunch<br> [[‘sandwich’, 0.95371924052636048]]<br>how was your lunch today?<br> [[‘greeting’, 0.99120883810944971], [‘sandwich’, 0.31626066870883057]]<br>Experiment with other sentences and different probabilities, you can then add training data and improve/expand the model. Notice the solid predictions with scant training data.<br>Some sentences will produce multiple predictions (above a threshold). You will need to establish the right threshold level for your application. Not all text classification scenarios are the same: some predictive situations require more confidence than others.<br>The last classification shows some internal details:<br>found in bag: good<br>found in bag: day<br>sentence: good day<br> bow: [0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]<br>good day<br> [[‘greeting’, 0.99664077655648697]]<br>Notice the bag-of-words (bow) for the sentence, 2 words matched our corpus. The neural-net also learns from the 0’s, the non-matching words.<br>A low-probability classification is easily shown by providing a sentence where ‘a’ (common word) is the only match, for example:<br>found in bag: a<br>sentence: a burrito!<br> bow: [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]<br>a burrito!<br> [[‘sandwich’, 0.61776860634647834]]<br>Here you have a fundamental piece of machinery for building a chat-bot, capable of handling a large # of classes (‘intents’) and suitable for classes with limited or extensive training data (‘patterns’). Adding one or more responses to an intent is trivial.<br>Enjoy!</p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Python/" rel="tag">#Python</a>
          
            <a href="/tags/ANN/" rel="tag">#ANN</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/08/17/36/" rel="next" title="Python Decorator 基础">
                <i class="fa fa-chevron-left"></i> Python Decorator 基础
              </a>
            
          </div>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/11/02/38/" rel="prev" title="How to update zsh to latest version on Amazon EC2 instance">
                How to update zsh to latest version on Amazon EC2 instance <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            Overview
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel ">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/avatar.jpg"
               alt="C0deZLee" />
          <p class="site-author-name" itemprop="name">C0deZLee</p>
          <p class="site-description motion-element" itemprop="description">Web Developer</p>
        </div>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">37</span>
              <span class="site-state-item-name">posts</span>
            </a>
          </div>

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">6</span>
                <span class="site-state-item-name">categories</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">25</span>
                <span class="site-state-item-name">tags</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/c0dezlee" target="_blank" title="Github">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  Github
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="mailto:c0dezlee@gmail.com" target="_blank" title="Email">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                  Email
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="https://www.linkedin.com/in/c0dez" target="_blank" title="LinkedIn">
                  
                    <i class="fa fa-fw fa-linkedin"></i>
                  
                  LinkedIn
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://stackoverflow.com/users/4985648/c0dez" target="_blank" title="StackOverFlow">
                  
                    <i class="fa fa-fw fa-stack-overflow"></i>
                  
                  StackOverFlow
                </a>
              </span>
            
          
        </div>

        
        

        
        

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">
            
              
            
            
              <p class="post-toc-empty">This post does not have a Table of Contents</p>
            
          </div>
        </section>
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2014 - 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">C0deZLee</span>
</div>

<div class="powered-by">
  Powered by <a class="theme-link" href="http://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  Theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>

        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  



  
  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/vendors/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.0.1"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.0.1"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.0.1"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.0.1"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.0.1"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.0.1"></script>



  



  




  
  

  

  

  

</body>
</html>
